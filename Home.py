import streamlit as st
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv, find_dotenv
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough
import fitz
import os

st.set_page_config(page_title="CEFET - Chat sobre o Cefet", page_icon="üéì")

# Bot√µes no topo
col1, col2 = st.columns([1, 1])
with col1:
    if st.button("üîê Login"):
        st.switch_page("pages/Login.py")


# Carrega as vari√°veis de ambiente
_ = load_dotenv(find_dotenv())

model = ChatGoogleGenerativeAI(model="gemini-2.0-flash")

def extrai_texto_para_pdf(pdf_path):
    text = ""
    doc = fitz.open(pdf_path)
    for page in doc:
        text += page.get_text("text") + "\n"
    return text

@st.cache_resource
def load_pdf_data():    
    pdf_path = "perguntas2.pdf"
    if not os.path.exists(pdf_path):
        st.error("Arquivo PDF n√£o encontrado!")
        return None
    
    texto_extraido = extrai_texto_para_pdf(pdf_path)
    document = Document(page_content=texto_extraido, metadata={"source": pdf_path})
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    vectorstore = FAISS.from_documents([document], embeddings)
    return vectorstore.as_retriever()

retriever = load_pdf_data()

st.title("üéì CEFET-MG - Assistente Virtual")

prompt = ChatPromptTemplate.from_template("""
Voc√™ √© um atendente virtual amig√°vel e prestativo de uma faculdade chamada CEFET-MG (Centro Federal de Educa√ß√£o Tecnol√≥gica de Minas Gerais) no campus de Varginha.
Seu trabalho √© fornecer informa√ß√µes sobre o curso de Sistemas de Informa√ß√£o de maneira educada, simp√°tica e clara.
Consultando as informa√ß√µes extra√≠das do texto, sempre seja organizado e detalhado.
Sempre seja gentil ao responder.
Contexto: {context}
Pergunta do cliente: {question}

""")

chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
)

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if user_input := st.chat_input("Digite sua d√∫vida"):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    response_stream = chain.stream(user_input)
    full_response = ""

    with st.chat_message("assistant"):
        response_box = st.empty()
        for partial in response_stream:
            full_response += str(partial.content)
            response_box.markdown(full_response)

    st.session_state.messages.append({"role": "assistant", "content": full_response})
